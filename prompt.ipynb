{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genenerate Dataset\n",
    "\n",
    "Using the deepseek, conver the extracted text preserving the chapters content as exact will be send to generate Q&A pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install requests dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_1.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_1.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_10.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_10.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_11.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_11.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_2.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_2.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_3.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_3.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_4.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_4.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_5.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_5.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_6.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_6.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_7.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_7.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_8.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_8.txt\n",
      "Waiting for 10 seconds before the next file...\n",
      "Processing data_9.txt...\n",
      "Error: 402 - {\"error\":{\"message\":\"Insufficient Balance\",\"type\":\"unknown_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n",
      "Failed to generate Q&A pairs for data_9.txt\n",
      "Waiting for 10 seconds before the next file...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Replace with your DeepSeek API key\n",
    "API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# DeepSeek API endpoint\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "\n",
    "# Input and output directories\n",
    "INPUT_DIR = \"./data/text_extraction_data\"\n",
    "OUTPUT_DIR = \"./data/json_dataset\"\n",
    "\n",
    "# Wait time between API calls (in seconds)\n",
    "WAIT_TIME = 10  # Adjust based on API rate limits\n",
    "\n",
    "# Function to generate Q&A pairs\n",
    "def generate_qa_pairs(text):\n",
    "    # Define the prompt for the API\n",
    "    prompt = f\"\"\"\n",
    "    Generate question-answer pairs in JSON format from the following text. \n",
    "    Ensure the questions are clear and the answers are concise and accurate.\n",
    "    The output should be a list of dictionaries, where each dictionary has two keys: \"question\" and \"answer\".\n",
    "\n",
    "    Example Output:\n",
    "    [\n",
    "      {{\n",
    "        \"question\": \"What is the purpose of DeepSeek's open-source initiative?\",\n",
    "        \"answer\": \"DeepSeek's open-source initiative aims to share production-ready tools and frameworks to accelerate AGI exploration. By open-sourcing their code, DeepSeek fosters collaboration, transparency, and innovation within the AI community.\"\n",
    "      }},\n",
    "      {{\n",
    "        \"question\": \"What is FlashMLA, and how is it optimized for Hopper GPUs?\",\n",
    "        \"answer\": \"FlashMLA is an efficient Multi-Head Latent Attention (MLA) decoding kernel optimized for Hopper GPUs. It supports BF16, uses a paged KV cache with a block size of 64, and achieves 3000 GB/s memory-bound performance and 580 TFLOPS compute-bound performance on H800 GPUs.\"\n",
    "      }}\n",
    "    ]\n",
    "\n",
    "    Text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the request payload\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-chat\",  # Specify the model\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates question-answer pairs in JSON format.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 2000,  # Adjust based on the length of the text\n",
    "        \"temperature\": 0.7,  # Controls creativity (0.7 is a good balance)\n",
    "    }\n",
    "\n",
    "    # Set up headers with the API key\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Send the request to the DeepSeek API\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response\n",
    "        response_data = response.json()\n",
    "        qa_pairs = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # Convert the response to a Python list of dictionaries\n",
    "        try:\n",
    "            qa_pairs = json.loads(qa_pairs)\n",
    "            return qa_pairs\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: The response is not valid JSON.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Function to process all input files\n",
    "def process_files(input_dir, output_dir, wait_time):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Get a list of all .txt files in the input directory\n",
    "    input_files = [f for f in os.listdir(input_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "    # Process each file sequentially\n",
    "    for file_name in input_files:\n",
    "        # Construct full file paths\n",
    "        input_path = os.path.join(input_dir, file_name)\n",
    "        output_path = os.path.join(output_dir, file_name.replace(\".txt\", \".json\"))\n",
    "\n",
    "        # Read the input file\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Generate Q&A pairs\n",
    "        print(f\"Processing {file_name}...\")\n",
    "        qa_pairs = generate_qa_pairs(text)\n",
    "\n",
    "        # Save the result as a JSON file\n",
    "        if qa_pairs:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(qa_pairs, f, indent=4)\n",
    "            print(f\"Saved Q&A pairs to {output_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to generate Q&A pairs for {file_name}\")\n",
    "\n",
    "        # Wait before processing the next file\n",
    "        print(f\"Waiting for {wait_time} seconds before the next file...\")\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "# Run the script\n",
    "process_files(INPUT_DIR, OUTPUT_DIR, WAIT_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all json data to single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added data_1.json to the combined dataset.\n",
      "Added data_10.json to the combined dataset.\n",
      "Added data_11.json to the combined dataset.\n",
      "Added data_12.json to the combined dataset.\n",
      "Added data_13.json to the combined dataset.\n",
      "Added data_14.json to the combined dataset.\n",
      "Added data_15.json to the combined dataset.\n",
      "Added data_2.json to the combined dataset.\n",
      "Added data_3.json to the combined dataset.\n",
      "Added data_4.json to the combined dataset.\n",
      "Added data_5.json to the combined dataset.\n",
      "Added data_6.json to the combined dataset.\n",
      "Added data_7.json to the combined dataset.\n",
      "Added data_8.json to the combined dataset.\n",
      "Added data_9.json to the combined dataset.\n",
      "Combined dataset saved to ./data/dataset.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Output directory containing individual JSON files\n",
    "OUTPUT_DIR = \"./data/json_dataset\"\n",
    "\n",
    "# Output file for the combined dataset\n",
    "COMBINED_OUTPUT_FILE = \"./data/dataset.json\"\n",
    "\n",
    "# Function to combine JSON files\n",
    "def combine_json_files(output_dir, combined_output_file):\n",
    "    # Initialize an empty list to store all Q&A pairs\n",
    "    combined_data = []\n",
    "\n",
    "    # Get a list of all .json files in the output directory\n",
    "    json_files = [f for f in os.listdir(output_dir) if f.endswith(\".json\")]\n",
    "\n",
    "    # Process each JSON file\n",
    "    for file_name in json_files:\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        # Read the JSON file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Append the data to the combined list\n",
    "        combined_data.extend(data)\n",
    "        print(f\"Added {file_name} to the combined dataset.\")\n",
    "\n",
    "    # Save the combined data to a single JSON file\n",
    "    with open(combined_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined_data, f, indent=4)\n",
    "    print(f\"Combined dataset saved to {combined_output_file}\")\n",
    "\n",
    "# Run the script\n",
    "combine_json_files(OUTPUT_DIR, COMBINED_OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fien-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.21.0-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.6.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-75.8.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Using cached torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached torchvision-0.21.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "Downloading torchaudio-2.6.0-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 728.2 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.8/2.4 MB 838.9 kB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.4 MB 838.9 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 898.8 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 898.8 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.3/2.4 MB 789.6 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.3/2.4 MB 789.6 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.3/2.4 MB 789.6 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.6/2.4 MB 665.7 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.8/2.4 MB 689.5 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 720.5 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 720.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 732.7 kB/s eta 0:00:00\n",
      "Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached setuptools-75.8.2-py3-none-any.whl (1.2 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, setuptools, pillow, networkx, MarkupSafe, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 pillow-11.1.0 setuptools-75.8.2 sympy-1.13.1 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: safetensors, tokenizers, transformers\n",
      "Successfully installed safetensors-0.5.3 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (0.29.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from aiohttp->datasets) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from peft) (4.49.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from peft) (4.67.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from peft) (0.29.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: requests in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch>=1.13.0->peft) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Installing collected packages: accelerate, peft\n",
      "Successfully installed accelerate-1.4.0 peft-0.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (0.45.3)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from bitsandbytes) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kenur\\documents\\python\\intellihack5\\venv\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "def finetune_qwen(train_file, model_name=\"Qwen/Qwen2.5-3B-Instruct\", output_dir=\"./qwen2.5-3b-research-qa-lora\"):\n",
    "    \"\"\"\n",
    "    Fine-tunes a Qwen model using a custom dataset.\n",
    "\n",
    "    Args:\n",
    "        train_file (str): Path to the JSON file containing the training data.\n",
    "        model_name (str): Name of the Qwen model to fine-tune.\n",
    "        output_dir (str): Output directory for saving the fine-tuned LoRA adapters.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load Model and Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        offload_folder=\"offload\",  # Offload to CPU\n",
    "        offload_state_dict=True,   # Offload state dict to CPU\n",
    "    )\n",
    "\n",
    "    # 2. Prepare Model for QLoRA\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"]  # Adjust for Qwen if needed\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    # 3. Load and Tokenize Dataset\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [f\"Question: {q} Answer: \" for q in examples[\"question\"]]  # Customize prompt\n",
    "        targets = [a for a in examples[\"answer\"]]\n",
    "        model_inputs = tokenizer(inputs, text_target=targets, max_length=512, truncation=True,\n",
    "                                   padding=\"max_length\")  # Adjust max_length\n",
    "        return model_inputs\n",
    "\n",
    "    try:\n",
    "        dataset = load_dataset(\"json\", data_files=train_file, split=\"train\") # Loading with key file location.\n",
    "        tokenized_train_dataset = dataset.map(preprocess_function, batched=True) # pre-processing is still batch processing\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Training data file not found at {train_file}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An error occured reading the JSON file {e}\")\n",
    "        return\n",
    "\n",
    "    # 4. Set up Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"no\",\n",
    "        fp16=True,  # Or bf16=True if your GPU supports it\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.05,\n",
    "        report_to=\"none\",\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    # 5. Create Trainer and Train\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # 6. Save Trained Model (LoRA Adapters)\n",
    "    model.save_pretrained(output_dir)\n",
    "    print(f\"Fine-tuning complete! LoRA adapters saved to {output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_data_file = \"./data/dataset.json\"  # Path to your JSON training data file\n",
    "    finetune_qwen(train_data_file)  # run model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the gguf File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"./qwen2.5-3b-research-qa-lora\"  # Path to your fine-tuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# Save the model in a format compatible with llama.cpp\n",
    "model.save_pretrained(\"./qwen-gguf\")\n",
    "tokenizer.save_pretrained(\"./qwen-gguf\")\n",
    "\n",
    "# Convert to gguf format using llama.cpp\n",
    "# Run this in your terminal after saving the model\n",
    "# python3 llama.cpp/convert.py --model ./qwen-gguf --outfile ./qwen-gguf/model.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Chatbot with In-Memory Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from llama_cpp import Llama\n",
    "import redis\n",
    "\n",
    "# Load the gguf model\n",
    "model_path = \"./qwen-gguf/model.gguf\"\n",
    "llm = Llama(model_path=model_path)\n",
    "\n",
    "# Initialize Redis for chat history\n",
    "redis_client = redis.Redis(host=\"localhost\", port=6379, db=0)\n",
    "\n",
    "# Function to generate a response\n",
    "def generate_response(prompt, chat_history):\n",
    "    # Combine chat history with the new prompt\n",
    "    full_prompt = \"\\n\".join(chat_history + [f\"User: {prompt}\"])\n",
    "    output = llm(full_prompt, max_tokens=512, stop=[\"User:\"], echo=False)\n",
    "    return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# Chat loop\n",
    "def chat():\n",
    "    chat_id = \"user_123\"  # Unique ID for the chat session\n",
    "    chat_history = redis_client.lrange(chat_id, 0, -1)  # Load chat history\n",
    "    chat_history = [msg.decode(\"utf-8\") for msg in chat_history]\n",
    "\n",
    "    print(\"Chatbot: Hello! How can I assist you today?\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Generate response\n",
    "        response = generate_response(user_input, chat_history)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "        # Update chat history\n",
    "        chat_history.append(f\"User: {user_input}\")\n",
    "        chat_history.append(f\"Chatbot: {response}\")\n",
    "        redis_client.rpush(chat_id, *chat_history[-2:])  # Save last two messages\n",
    "\n",
    "# Start the chat\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
